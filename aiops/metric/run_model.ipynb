{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4343a28f-9f4f-4854-8df6-04833738189e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import List\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from grpclib.client import Channel\n",
    "from cortexadmin_pb import CortexAdminStub, QueryRangeRequest, QueryRequest, MatcherRequest\n",
    "from betterproto.lib.google.protobuf import Empty\n",
    "\n",
    "default_query_interval = \"1m\"\n",
    "\n",
    "async def get_all_users(service: CortexAdminStub) -> List[str]:\n",
    "  response = await service.all_user_stats(Empty())\n",
    "  return [r.user_id for r in response.items]\n",
    "\n",
    "async def list_all_metric(service: CortexAdminStub, cluster_id: str) -> List[str]:\n",
    "  response = await service.extract_raw_series(MatcherRequest(tenant=cluster_id, match_expr=\".+\"))\n",
    "  res = (json.loads(response.data.decode())[\"data\"])\n",
    "  s = set()\n",
    "  for r in res[\"result\"]:\n",
    "    s.add(r[\"metric\"][\"__name__\"])\n",
    "  return list(s)\n",
    "\n",
    "async def metric_query(service: CortexAdminStub, cluster_id: str, metric_name: str, namespace=\"opni\"):\n",
    "  query = f'sum(rate({metric_name}{{namespace=\"{namespace}\"}}[{default_query_interval}])) by (pod)'\n",
    "  response = await service.query(QueryRequest(tenants=[cluster_id], query=query))\n",
    "  response = json.loads(response.data.decode())[\"data\"]\n",
    "  return response\n",
    "\n",
    "async def metric_queryrange(service: CortexAdminStub, cluster_id: str, metric_name: str, namespace=\"opni\", end_time : datetime = None, time_delta : timedelta= timedelta(minutes=60), step_minute : int = 1):\n",
    "  query_interval = \"2m\"# f\"{step_minute}m\"\n",
    "  query = f'sum(rate({metric_name}{{namespace=\"{namespace}\"}}[{query_interval}])) by (pod)'\n",
    "  if end_time is None:\n",
    "    end_time = datetime.now()\n",
    "  start_time = end_time - time_delta\n",
    "  response = await service.query_range(QueryRangeRequest(tenants=[cluster_id], query=query, start=start_time, end=end_time, step=timedelta(minutes=step_minute)))\n",
    "  response = json.loads(response.data.decode())[\"data\"]\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b29479-b758-4d86-8102-0f65961a23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = Channel(host=\"localhost\", port=11090) # url of opni-internal. can port-forward to localhost:11090\n",
    "service = CortexAdminStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ba21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plt_plot(ts_data, title=None):\n",
    "    # Plot the time series data\n",
    "    if title is None:\n",
    "        title = \"Time Series Data\"\n",
    "    plt.plot(ts_data)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071b61b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp, ks_1samp, mannwhitneyu, ttest_ind\n",
    "from collections import defaultdict\n",
    "\n",
    "threshold = 0.05\n",
    "\n",
    "def moving_average(data, window_size=2):\n",
    "    data = np.array(data)\n",
    "\n",
    "    weights = np.repeat(1.0, window_size) / window_size\n",
    "    moving_avg = np.convolve(data, weights, 'valid')\n",
    "    return moving_avg\n",
    "\n",
    "def ks_anomaly_detection(l1, l2 ):\n",
    "    # metric_values = moving_average(metric_values)\n",
    "    ks_stat, p_value = ks_2samp(l1, l2)\n",
    "    if p_value < threshold:\n",
    "        return True, p_value\n",
    "    else:\n",
    "        return False, p_value\n",
    "\n",
    "def mw_anomaly_detection(l1, l2):\n",
    "    # metric_values = moving_average(metric_values)\n",
    "    stat, p_value = mannwhitneyu(l1, l2)\n",
    "    if p_value < threshold:\n",
    "        return True, p_value\n",
    "    else:\n",
    "        return False, p_value\n",
    "\n",
    "def ttest_anomaly_detection(l1, l2):\n",
    "    # metric_values = moving_average(metric_values)\n",
    "    stat, p_value = ttest_ind(l1, l2)\n",
    "    if p_value < threshold:\n",
    "        return True, p_value\n",
    "    else:\n",
    "        return False, p_value\n",
    "\n",
    "def zscore_anomaly_detection(metric_values: List[float], test_values):\n",
    "    mean = np.mean(metric_values)\n",
    "    std_dev = np.std(metric_values)\n",
    "\n",
    "    # set a threshold value\n",
    "    threshold = 3\n",
    "\n",
    "    # identify anomalies using z-score\n",
    "    anomalies = []\n",
    "    for x in test_values:\n",
    "        z_score = abs((x - mean) / std_dev)\n",
    "        if z_score > threshold:\n",
    "            anomalies.append(x)\n",
    "    return anomalies\n",
    "\n",
    "import time \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f83a006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 0\n",
    "user_id = (await get_all_users(service))[cluster]\n",
    "\n",
    "metrics = await list_all_metric(service, user_id)\n",
    "\n",
    "fixed_ts = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "694e75b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adservice',\n",
       " 'cartservice',\n",
       " 'checkoutservice',\n",
       " 'currencyservice',\n",
       " 'emailservice',\n",
       " 'frontend',\n",
       " 'frontend-external',\n",
       " 'kubernetes',\n",
       " 'paymentservice',\n",
       " 'productcatalogservice',\n",
       " 'recommendationservice',\n",
       " 'redis-cart',\n",
       " 'shippingservice']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def list_namespace(cluster_id):\n",
    "    query = \"kube_namespace_labels\"\n",
    "    response = await service.query(QueryRequest(tenants=[cluster_id], query=query))\n",
    "    response = json.loads(response.data.decode())[\"data\"]\n",
    "    res = [i[\"metric\"]['namespace'] for i in response[\"result\"]]\n",
    "    return res\n",
    "\n",
    "async def list_ns_pod(cluster_id, namespace):\n",
    "    query = f'kube_pod_labels{{namespace=\"{namespace}\"}}'\n",
    "    response = await service.query(QueryRequest(tenants=[cluster_id], query=query))\n",
    "    response = json.loads(response.data.decode())[\"data\"]\n",
    "    res = [i[\"metric\"]['pod'] for i in response[\"result\"]]\n",
    "    return res\n",
    "\n",
    "async def list_ns_service(cluster_id, namespace):\n",
    "    query = f'kube_service_labels{{namespace=\"{namespace}\"}}'\n",
    "    response = await service.query(QueryRequest(tenants=[cluster_id], query=query))\n",
    "    response = json.loads(response.data.decode())[\"data\"]\n",
    "    res = [i[\"metric\"]['service'] for i in response[\"result\"]]\n",
    "    return res\n",
    "\n",
    "res = await list_ns_service(user_id, \"default\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b924cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec6bf70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 4, 7, 15, 6, 0, 361902)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "475263e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum(rate(kubelet_eviction_stats_age_seconds_count{namespace=\"default\"}[2m])) by (pod)\n",
      "sum(rate(workqueue_queue_duration_seconds_count{namespace=\"default\"}[2m])) by (pod)\n",
      "sum(rate(node_network_receive_packets_total{namespace=\"default\"}[2m])) by (pod)\n",
      "sum(rate(apiserver_storage_list_evaluated_objects_total{namespace=\"default\"}[2m])) by (pod)\n",
      "sum(rate(kubelet_pod_start_duration_seconds_bucket{namespace=\"default\"}[2m])) by (pod)\n",
      "sum(rate(node_netstat_TcpExt_TCPSynRetrans{namespace=\"default\"}[2m])) by (pod)\n",
      "sum(rate(rest_client_exec_plugin_certificate_rotation_age_count{namespace=\"default\"}[2m])) by (pod)\n",
      "sum(rate(jaeger_tracer_reporter_spans_total{namespace=\"default\"}[2m])) by (pod)\n",
      "sum(rate(node_netstat_Icmp_InErrors{namespace=\"default\"}[2m])) by (pod)\n",
      "sum(rate(prometheus_remote_storage_samples_failed_total{namespace=\"default\"}[2m])) by (pod)\n",
      "sum(rate(kube_namespace_labels{namespace=\"default\"}[2m])) by (pod)\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m         res\u001b[39m.\u001b[39mappend(q1)\n\u001b[1;32m     10\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m---> 11\u001b[0m qs \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m pull_metrics_data(fixed_ts, service, user_id, metrics, ns\u001b[39m=\u001b[39mcluster_ns)\n",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m, in \u001b[0;36mpull_metrics_data\u001b[0;34m(end_time, service, user_id, metrics, ns)\u001b[0m\n\u001b[1;32m      6\u001b[0m         res\u001b[39m.\u001b[39mappend([])\n\u001b[1;32m      7\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     q1 \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m metric_queryrange(service, user_id, m,end_time\u001b[39m=\u001b[39mend_time, time_delta\u001b[39m=\u001b[39mtimedelta(minutes\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m),step_minute\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, namespace\u001b[39m=\u001b[39mns)\n\u001b[1;32m      9\u001b[0m     res\u001b[39m.\u001b[39mappend(q1)\n\u001b[1;32m     10\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "Cell \u001b[0;32mIn[14], line 37\u001b[0m, in \u001b[0;36mmetric_queryrange\u001b[0;34m(service, cluster_id, metric_name, namespace, end_time, time_delta, step_minute)\u001b[0m\n\u001b[1;32m     35\u001b[0m   end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m     36\u001b[0m start_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m time_delta\n\u001b[0;32m---> 37\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m service\u001b[39m.\u001b[39mquery_range(QueryRangeRequest(tenants\u001b[39m=\u001b[39m[cluster_id], query\u001b[39m=\u001b[39mquery, start\u001b[39m=\u001b[39mstart_time, end\u001b[39m=\u001b[39mend_time, step\u001b[39m=\u001b[39mtimedelta(minutes\u001b[39m=\u001b[39mstep_minute)))\n\u001b[1;32m     38\u001b[0m response \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(response\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdecode())[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/workspace/opni/aiops/metric/cortexadmin_pb.py:323\u001b[0m, in \u001b[0;36mCortexAdminStub.query_range\u001b[0;34m(self, query_range_request, timeout, deadline, metadata)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mquery_range\u001b[39m(\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    318\u001b[0m     query_range_request: QueryRangeRequest,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m     metadata: Optional[MetadataLike] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    322\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m QueryResponse:\n\u001b[0;32m--> 323\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unary_unary(\n\u001b[1;32m    324\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m/cortexadmin.CortexAdmin/QueryRange\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    325\u001b[0m         query_range_request,\n\u001b[1;32m    326\u001b[0m         QueryResponse,\n\u001b[1;32m    327\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    328\u001b[0m         deadline\u001b[39m=\u001b[39mdeadline,\n\u001b[1;32m    329\u001b[0m         metadata\u001b[39m=\u001b[39mmetadata,\n\u001b[1;32m    330\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/py38metric/lib/python3.8/site-packages/betterproto/grpc/grpclib_client.py:85\u001b[0m, in \u001b[0;36mServiceStub._unary_unary\u001b[0;34m(self, route, request, response_type, timeout, deadline, metadata)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m     78\u001b[0m     route,\n\u001b[1;32m     79\u001b[0m     grpclib\u001b[39m.\u001b[39mconst\u001b[39m.\u001b[39mCardinality\u001b[39m.\u001b[39mUNARY_UNARY,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__resolve_request_kwargs(timeout, deadline, metadata),\n\u001b[1;32m     83\u001b[0m ) \u001b[39mas\u001b[39;00m stream:\n\u001b[1;32m     84\u001b[0m     \u001b[39mawait\u001b[39;00m stream\u001b[39m.\u001b[39msend_message(request, end\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 85\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m stream\u001b[39m.\u001b[39mrecv_message()\n\u001b[1;32m     86\u001b[0m \u001b[39massert\u001b[39;00m response \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/py38metric/lib/python3.8/site-packages/grpclib/client.py:425\u001b[0m, in \u001b[0;36mStream.recv_message\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Coroutine to receive incoming message from the server.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \n\u001b[1;32m    402\u001b[0m \u001b[39mIf server sends UNARY response, then you can call this coroutine only\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m:returns: message\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recv_initial_metadata_done:\n\u001b[0;32m--> 425\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecv_initial_metadata()\n\u001b[1;32m    427\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapper:\n\u001b[1;32m    428\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m recv_message(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_codec,\n\u001b[1;32m    429\u001b[0m                                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recv_type)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/py38metric/lib/python3.8/site-packages/grpclib/client.py:368\u001b[0m, in \u001b[0;36mStream.recv_initial_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[39mraise\u001b[39;00m ProtocolError(\u001b[39m'\u001b[39m\u001b[39mInitial metadata was already received\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    367\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrapper:\n\u001b[0;32m--> 368\u001b[0m     headers \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream\u001b[39m.\u001b[39mrecv_headers()\n\u001b[1;32m    369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recv_initial_metadata_done \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    370\u001b[0m     headers_map \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(headers)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/envs/py38metric/lib/python3.8/site-packages/grpclib/protocol.py:342\u001b[0m, in \u001b[0;36mStream.recv_headers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mrecv_headers\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _Headers:\n\u001b[1;32m    341\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 342\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders_received\u001b[39m.\u001b[39mwait()\n\u001b[1;32m    343\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheaders\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.6/lib/python3.8/asyncio/locks.py:309\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_waiters\u001b[39m.\u001b[39mappend(fut)\n\u001b[1;32m    308\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m     \u001b[39mawait\u001b[39;00m fut\n\u001b[1;32m    310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cluster_ns = \"default\"\n",
    "async def pull_metrics_data(end_time, service, user_id:str, metrics, ns=\"opni\"):\n",
    "    res = []\n",
    "    for m in metrics:\n",
    "        if \"cortex\" in m:\n",
    "            res.append([])\n",
    "            continue\n",
    "        q1 = await metric_queryrange(service, user_id, m,end_time=end_time, time_delta=timedelta(minutes=300),step_minute=1, namespace=ns)\n",
    "        res.append(q1)\n",
    "    return res\n",
    "qs = await pull_metrics_data(fixed_ts, service, user_id, metrics, ns=cluster_ns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364a2afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1135\n"
     ]
    }
   ],
   "source": [
    "imrpove_data = []\n",
    "from model.data_simulator import normalize_format\n",
    "def process_task2(d, q1 ,m_name, is_debug = False):\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for r in q1[\"result\"]:\n",
    "        if \"pod\" not in r[\"metric\"]:\n",
    "            # print(r)\n",
    "            continue\n",
    "        pod = r[\"metric\"][\"pod\"]\n",
    "        # list0 = r[\"values\"]\n",
    "        values0 = [float(l[1]) for l in r[\"values\"]]\n",
    "        history, evaluate_window, test_window = values0[:240], values0[-60:-10], values0[-10:]\n",
    "        try:\n",
    "            is_anomaly, p_value = ttest_anomaly_detection(evaluate_window, values0[-10:])\n",
    "            total += 1\n",
    "            # if \"network\" in m_name and \"payment\" in pod:\n",
    "            #     plt_plot(values0[-60:])\n",
    "            #     print(pod)\n",
    "            #     print(m_name)\n",
    "            #     print(is_anomaly)\n",
    "            #     print(p_value)\n",
    "\n",
    "            if is_anomaly:\n",
    "            # if True:\n",
    "                mean = np.mean(history)\n",
    "                std_dev = np.std(history)\n",
    "                std_multiplier = 3\n",
    "                rule1 = max(test_window) > mean + std_multiplier * std_dev or min(test_window) < mean - std_multiplier * std_dev\n",
    "                rule2 = max(test_window) > max(history) or min(test_window) < min(history)\n",
    "                rule3 = np.mean(test_window) > mean + std_multiplier * std_dev or np.mean(test_window) < mean - std_multiplier * std_dev\n",
    "                rule4, p_4 = ks_anomaly_detection(values0[:150], values0[150:])\n",
    "                if rule1 and rule3:# and rule3 and rule4: # rule1 and rule2\n",
    "                    # z_s = zscore_anomaly_detection(values0)\n",
    "                    # z_s_binary = True if len(z_s) > 0 else False\n",
    "                    # if z_s_binary:\n",
    "                    count += 1\n",
    "                    d[pod+\"|\"+m_name] = values0[-60:]\n",
    "                    imrpove_data.append(values0[-60:])\n",
    "                    if is_debug:\n",
    "                    # if True:\n",
    "                        \n",
    "                        print(f\"metric_name: {m_name}, pod : {pod}, rule3: {rule3}, rule4: {rule4}\")\n",
    "                        plt_plot(np.array(values0), title=pod+\"-\"+m_name)\n",
    "                        print(is_anomaly)\n",
    "                        print(\"=====================\")\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    s2 = time.time()\n",
    "    return count, total\n",
    "\n",
    "d = defaultdict(list)\n",
    "total_count = 0\n",
    "total = 0\n",
    "for i,q in enumerate(qs):\n",
    "    if len(q) == 0:\n",
    "        continue\n",
    "    c, t = process_task2(d,q, metrics[i])\n",
    "    total_count += c\n",
    "    total += t\n",
    "print(total_count)\n",
    "print(total)\n",
    "# print(imrpove_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3decd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if total_count > 50:\n",
    "#     with open(\"collected_data/\" + str(fixed_ts.timestamp())+user_id+\".data\", 'w') as fout:\n",
    "#         fout.write(json.dumps(imrpove_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "410a4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.cnn_model import MpcModel\n",
    "from model.data_simulator import simulate_data, normalize_format\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, SGD\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    batch_size = 32\n",
    "    train_data = simulate_data(1000)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    total_batch = len(train_loader)\n",
    "    n_epoch = 400 \n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 0.0005 #1e-4, 1e-3, 1e-2\n",
    "\n",
    "    seed = 1234\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = MpcModel() # random initialization.\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for e in range(n_epoch):\n",
    "        print(f\"epoch : {e}\")\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch.to(device))\n",
    "            loss = loss_fn(y_pred.cpu(), y_batch)\n",
    "            # optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"total loss : {total_loss}, average loss : {total_loss / total_batch}\")\n",
    "\n",
    "        ## eval?\n",
    "\n",
    "    torch.save(model.state_dict() ,\"model.pth\")\n",
    "    return model\n",
    "\n",
    "def eval_model(test_data = None):\n",
    "    if not test_data:\n",
    "        test_data = simulate_data(100)\n",
    "    test_loader = DataLoader(test_data, batch_size=1, shuffle=False)\n",
    "    num_correct, num_total = 0, 0\n",
    "    mistakes = []\n",
    "    model = MpcModel()\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x_batch, y_batch) in enumerate(test_loader):\n",
    "            y_pred = model(x_batch.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).cpu()\n",
    "            nc = torch.sum(y_pred == y_batch).item()\n",
    "            num_correct += nc\n",
    "            nt = y_batch.size(0)\n",
    "            num_total += nt\n",
    "            if nc == 0:\n",
    "                mistakes.append(i)\n",
    "\n",
    "    accuracy = num_correct / num_total\n",
    "    print(f\"accuracy : {accuracy}\")\n",
    "    print(f\"mistakes : {mistakes}\")\n",
    "    print(len(mistakes))\n",
    "    print(num_total)\n",
    "\n",
    "def predict(pred_data):\n",
    "    pred_data = [torch.tensor(np.array([normalize_format(p)]), dtype=torch.float32) for p in pred_data]\n",
    "    test_loader = DataLoader(pred_data, batch_size=1, shuffle=False)\n",
    "    model = MpcModel()\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(\"model.pth\" , map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    res = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, x_batch in enumerate(test_loader):\n",
    "            y_pred = model(x_batch.to(device))\n",
    "            y_pred = torch.argmax(y_pred, dim=1).cpu()\n",
    "            res.append(int(y_pred))\n",
    "    return res\n",
    "\n",
    "def normalize_format(ts_data):\n",
    "    from sklearn.preprocessing import minmax_scale\n",
    "    res = minmax_scale(ts_data)\n",
    "    return res\n",
    "\n",
    "pred_data0 = np.array([d[k] for k in d])\n",
    "keys = [k for k in d]\n",
    "\n",
    "res = predict(pred_data0)\n",
    "\n",
    "pred_map = {\n",
    "      0: \"type1_level_shift_up\",\n",
    "      1: \"type1_level_shift_down\",\n",
    "      2: \"type1_steady_increase\",\n",
    "      3: \"type1_steady_decrease\",\n",
    "      4: \"type1_sudden_increase\",\n",
    "      5:\"type1_sudden_decrease\",\n",
    "      6: \"type2_single_spike\",\n",
    "      7: \"type2_single_dip\",\n",
    "      8: \"type2_multi_spike\",\n",
    "      9:\"type2_multi_dip\",\n",
    "      10:\"type2_transient_level_up\",\n",
    "      11:\"type2_transient_level_down\",\n",
    "      12: \"type2_fluctuations\",\n",
    "}\n",
    "res = [pred_map[r] for r in res]\n",
    "for i, p in enumerate(pred_data0):\n",
    "    print(res[i])\n",
    "    plt_plot(p, title=keys[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475ea7c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prometheus_http_requests_total',\n",
       " 'prometheus_operator_kubernetes_client_http_request_duration_seconds_count',\n",
       " 'kubelet_http_requests_duration_seconds_sum',\n",
       " 'promhttp_metric_handler_requests_total',\n",
       " 'prometheus_http_response_size_bytes_bucket',\n",
       " 'kubelet_http_requests_total',\n",
       " 'promhttp_metric_handler_errors_total',\n",
       " 'prometheus_sd_kubernetes_http_request_duration_seconds_count',\n",
       " 'prometheus_http_request_duration_seconds_sum',\n",
       " 'kubelet_http_inflight_requests',\n",
       " 'kubelet_http_requests_duration_seconds_count',\n",
       " 'prometheus_http_request_duration_seconds_count',\n",
       " 'prometheus_sd_kubernetes_http_request_total',\n",
       " 'prometheus_operator_kubernetes_client_http_request_duration_seconds_sum',\n",
       " 'promhttp_metric_handler_requests_in_flight',\n",
       " 'prometheus_sd_kubernetes_http_request_duration_seconds_sum',\n",
       " 'prometheus_http_response_size_bytes_count',\n",
       " 'kubelet_http_requests_duration_seconds_bucket',\n",
       " 'prometheus_http_response_size_bytes_sum',\n",
       " 'prometheus_http_request_duration_seconds_bucket',\n",
       " 'opni_gateway_http_requests_total',\n",
       " 'prometheus_sd_http_failures_total',\n",
       " 'prometheus_operator_kubernetes_client_http_requests_total']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx = [m for m in metrics if \"http\" in m]\n",
    "mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f52f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # m = \"container_network_receive_bytes_total\"\n",
    "# m = \"container_cpu_usage_seconds_total\"\n",
    "# # m = 'container_memory_usage_bytes'\n",
    "# print(user_id)\n",
    "\n",
    "# async def metric_queryrange1(service: CortexAdminStub, cluster_id: str, metric_name: str, namespace=\"opni\", end_time : datetime = None, time_delta : timedelta= timedelta(minutes=60), step_minute : int = 1):\n",
    "#   query_interval = \"2m\"# f\"{step_minute}m\"\n",
    "#   query = f'sum(rate({metric_name}{{namespace=\"{namespace}\"}}[{query_interval}]))  by (pod)'\n",
    "#   if end_time is None:\n",
    "#     end_time = datetime.now()\n",
    "#   start_time = end_time - time_delta\n",
    "#   response = await service.query_range(QueryRangeRequest(tenants=[cluster_id], query=query, start=start_time, end=end_time, step=timedelta(minutes=step_minute)))\n",
    "#   response = json.loads(response.data.decode())[\"data\"]\n",
    "#   return response\n",
    "# res = await metric_queryrange1(service, user_id, m, end_time=fixed_ts,namespace=\"default\")\n",
    "# res = res[\"result\"]\n",
    "# res\n",
    "# for r in res:\n",
    "#     v = [float(x[1]) for x in r[\"values\"]]\n",
    "#     print(r[\"metric\"])\n",
    "#     plt_plot(v)\n",
    "#     pred = predict([v])\n",
    "#     print(pred_map[int(pred[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "93d838bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the URL for the Grafana API\n",
    "grafana_url = \"http://aa7f185a486744f7ea262a19d7c4bed1-231505684.us-west-1.elb.amazonaws.com:3000/api\"\n",
    "\n",
    "# Set the headers for the API request\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': 'Bearer eyJrIjoia1Zwbm5Wb2dyazZkSmdEaW56WlhyNjJXNUZraFVMbHgiLCJuIjoiYWRtaW5rZXkiLCJpZCI6MX0='\n",
    "}\n",
    "\n",
    "def remove_dashboard(dashboard_uid):\n",
    "    url = grafana_url + '/dashboards/uid/' + dashboard_uid\n",
    "    # Send the delete request\n",
    "    response = requests.delete(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print('Dashboard deleted successfully')\n",
    "    else:\n",
    "        print('Failed to delete dashboard: {}'.format(response.text))\n",
    "\n",
    "def create_dashboard(dashboard_payload):\n",
    "    # Send a POST request to create the new dashboard\n",
    "    response = requests.post(grafana_url + '/dashboards/db', headers=headers, json=dashboard_payload)\n",
    "\n",
    "    # Verify that the dashboard was created successfully\n",
    "    if response.status_code == 200:\n",
    "        print('Dashboard created successfully')\n",
    "    else:\n",
    "        print('Failed to create dashboard: ' + response.text)\n",
    "\n",
    "\n",
    "# Define the JSON payload for the new dashboard\n",
    "dashboard_payload = {\n",
    "    'dashboard': {\n",
    "        'id': None,\n",
    "        'uid': \"dynamic-metricAI\",\n",
    "        'title': 'YB Dashboard',\n",
    "        'tags': ['dynamic example'],\n",
    "        'timezone': 'browser',\n",
    "        'schemaVersion': 22,\n",
    "        'version': 0,\n",
    "        'refresh': '30s',\n",
    "        'time': {\n",
    "            'from': 'now-1h',\n",
    "            'to': 'now'\n",
    "        },\n",
    "        'panels': [{\n",
    "            'id': 1,\n",
    "            'type': 'graph',\n",
    "            'title': 'this Panel',\n",
    "            'datasource': 'default',\n",
    "            'targets': [{\n",
    "                'refId': 'A',\n",
    "                \"expr\": \"sum(rate(container_cpu_usage_seconds_total{namespace=\\\"default\\\"}[2m])) by(pod)\"\n",
    "            }],\n",
    "            'xaxis': {\n",
    "                'show': True\n",
    "            },\n",
    "            'yaxes': [{\n",
    "                'format': 'short',\n",
    "                'label': 'Count',\n",
    "                'logBase': 1,\n",
    "                'max': None,\n",
    "                'min': 0,\n",
    "                'show': True\n",
    "            }, {\n",
    "                'format': 'short',\n",
    "                'label': None,\n",
    "                'logBase': 1,\n",
    "                'max': None,\n",
    "                'min': None,\n",
    "                'show': False\n",
    "            }],\n",
    "            'legend': {\n",
    "                'alignAsTable': True,\n",
    "                'avg': True,\n",
    "                'current': True,\n",
    "                'hideEmpty': False,\n",
    "                'hideZero': False,\n",
    "                'max': True,\n",
    "                'min': True,\n",
    "                'rightSide': True,\n",
    "                'show': True,\n",
    "                'sortDesc': True,\n",
    "                'sort': 'total',\n",
    "                'total': True,\n",
    "                'values': True\n",
    "            },\n",
    "            'gridPos': {\n",
    "                'h': 8,\n",
    "                'w': 12,\n",
    "                'x': 0,\n",
    "                'y': 0\n",
    "            },\n",
    "            'tooltip': {\n",
    "                'shared': True,\n",
    "                'sort': 0,\n",
    "                'value_type': 'individual'\n",
    "            },\n",
    "            'links': [],\n",
    "            'maxDataPoints': 100,\n",
    "            'nullPointMode': 'null',\n",
    "            'pointradius': 5,\n",
    "            'stack': False,\n",
    "            'steppedLine': False,\n",
    "            'timeFrom': None,\n",
    "            'timeShift': None,\n",
    "            'options': {\n",
    "                'showThresholdLabels': False,\n",
    "                'showThresholdMarkers': True\n",
    "            },\n",
    "            'pluginVersion': '7.4.3',\n",
    "            'thresholds': []\n",
    "        }]\n",
    "    },\n",
    "    'folderId': 34,\n",
    "    'overwrite': False\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "534506e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard created successfully\n"
     ]
    }
   ],
   "source": [
    "create_dashboard(dashboard_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19a90e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard deleted successfully\n"
     ]
    }
   ],
   "source": [
    "remove_dashboard(\"dynamic-metricAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36a397fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ID: 34, Folder Title: Dynamic Folder\n"
     ]
    }
   ],
   "source": [
    "def create_folder():\n",
    "\n",
    "    # Grafana API endpoint for creating a folder\n",
    "    create_folder_url = grafana_url + '/folders'\n",
    "\n",
    "    # Folder name and parent folder ID\n",
    "    folder_name = 'Dynamic Folder'\n",
    "    parent_folder_id = 0\n",
    "\n",
    "    data = {\n",
    "        'title': folder_name,\n",
    "        'parentId': parent_folder_id\n",
    "    }\n",
    "\n",
    "    # Send the HTTP request to create the folder\n",
    "    response = requests.post(create_folder_url, headers=headers, json=data)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        print('Folder created successfully.')\n",
    "    else:\n",
    "        print('Error creating folder.')\n",
    "        print(response.content)\n",
    "\n",
    "def list_folder():\n",
    "    get_folders_url = grafana_url + '/folders'\n",
    "    response = requests.get(get_folders_url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        folders = response.json()\n",
    "\n",
    "        # Print the ID and title of each folder\n",
    "        for folder in folders:\n",
    "            print(f\"Folder ID: {folder['id']}, Folder Title: {folder['title']}\")\n",
    "    else:\n",
    "        print('Error getting folder list.')\n",
    "        print(response.content)\n",
    "\n",
    "list_folder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38metric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "12743c56a1c6fb9d573a46d6810fe43abfc03eb020d53abd820a2eab96377291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
